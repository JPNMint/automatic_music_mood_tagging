

Dataset description 

Try different arch, learn pytorch

short chunk
do Transfer learning

try to improve, data aug, data comb, architecture etc.

THINK:
HOW TO EVALUATE scores
How to compare with other works


ASK RICHARD VOGL:

EMMA DATASET INFORMATION
How to set lower hop size etc, what material I should read
(fft_hop_size * (seq_len - 1) + feat_window_size) ? window_size aendert nicht viel 
When Transfer learning, does the architecture have to look like the model I am taking the weights (probably yes)
Window size... wieviel frames mitgenommen werden wegen INFORMATION

44100 sample rate /441 frames output 
Das mal 100 ist 1 sekunden


Output frame rate spectrogram
How to compare with other works. Is it even needed? At certain score classify as x.




Plan

try state of the art architectures (with window sizes)
try to use data augmentation
try transfer learning

But why? 
Hilft ein guter classifier das projekt?
Enough to have lower error?




architecture, architecture_file = get_architecture(cfg.model.architecture, None)

model = architecture(audio_input=cfg.model.audio_inputs, num_classes=9, debug=False, **cfg.features)


pretrained_dict = torch.load('/home/ykinoshita/humrec_mood_tagger/mood_tagger_master_thesis_V2/architectures/SotA/musicnn_sota.pth')
model_dict = model.state_dict()

# 1. filter out unnecessary keys
pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}
# 2. overwrite entries in the existing state dict
model_dict.update(pretrained_dict) 
# 3. load the new state dict
model.load_state_dict(pretrained_dict)





#TODO
1. write transfer learning arch
2. try musicnn and other
3. add data aug fine tuning etc

https://github.com/PandoraMedia/music-audio-representations




resampling TODO

https://github.com/jafetgado/resreg

https://arxiv.org/pdf/2102.09554.pdf#cite.branco2017smogn

MEETING Nov
Kann man aus niedrigere LR heraussehen welcher model bsser ist?